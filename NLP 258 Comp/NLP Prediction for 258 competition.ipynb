{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":74565,"databundleVersionId":8135531,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#imports\nimport pickle\nimport pandas as pd\nimport itertools\nfrom collections import Counter\nimport numpy as np\nfrom gensim.models import Word2Vec\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n\n\nimport nltk\nfrom nltk.corpus import stopwords, wordnet\nnltk.download('punkt') \nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom gensim.models import word2vec\n\nimport string\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n  \n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-26T07:28:08.756703Z","iopub.execute_input":"2024-05-26T07:28:08.757222Z","iopub.status.idle":"2024-05-26T07:28:28.819514Z","shell.execute_reply.started":"2024-05-26T07:28:08.757165Z","shell.execute_reply":"2024-05-26T07:28:28.818132Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n[nltk_data]     failure in name resolution>\n/kaggle/input/ucsd-dsc-258r-nlp/train.csv\n/kaggle/input/ucsd-dsc-258r-nlp/test.csv\n/kaggle/input/ucsd-dsc-258r-nlp/baseline.ipynb\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"# A function used to build a vocabulary based on descending word frequencies \ndef build_vocab(sentences):\n    # Build vocabulary\n    word_counts = Counter(itertools.chain(*sentences))\n    # Mapping from index to word\n    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n    # Mapping from word to index\n    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n    return word_counts, vocabulary, vocabulary_inv","metadata":{"execution":{"iopub.status.busy":"2024-05-26T07:28:28.821757Z","iopub.execute_input":"2024-05-26T07:28:28.822216Z","iopub.status.idle":"2024-05-26T07:28:28.828503Z","shell.execute_reply.started":"2024-05-26T07:28:28.822155Z","shell.execute_reply":"2024-05-26T07:28:28.827373Z"},"trusted":true},"outputs":[],"execution_count":59},{"cell_type":"code","source":"def get_embeddings(inp_data, vocabulary_inv, size_features=100, mode='skipgram', min_word_count=2, context=5):\n    model_name = \"embedding\"\n    num_workers = 15\n    downsampling = 1e-3\n    print('Training Word2Vec model...')\n    sentences = [[vocabulary_inv[w] for w in s if w != -1] for s in inp_data]\n    sg = 1 if mode == 'skipgram' else 0\n    embedding_model = Word2Vec(sentences, workers=num_workers, sg=sg, vector_size=size_features, min_count=min_word_count, window=context, sample=downsampling)\n    print(\"Saving Word2Vec model {}\".format(model_name))\n    embedding_weights = np.zeros((len(vocabulary_inv), size_features))\n    for i in range(len(vocabulary_inv)):\n        word = vocabulary_inv[i]\n        if word in embedding_model.wv:\n            embedding_weights[i] = embedding_model.wv[word]\n        else:\n            embedding_weights[i] = np.random.uniform(-0.25, 0.25, embedding_model.vector_size)\n    return embedding_model","metadata":{"execution":{"iopub.status.busy":"2024-05-26T07:28:28.829786Z","iopub.execute_input":"2024-05-26T07:28:28.830058Z","iopub.status.idle":"2024-05-26T07:28:28.840498Z","shell.execute_reply.started":"2024-05-26T07:28:28.830033Z","shell.execute_reply":"2024-05-26T07:28:28.839535Z"},"trusted":true},"outputs":[],"execution_count":60},{"cell_type":"code","source":"def preprocess_df(df):\n    # get English stopwords\n    stop_words = set(stopwords.words('english'))\n    stop_words.add('would')\n    # prepare translation table to translate punctuation to space\n    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n    preprocessed_sentences = []\n    for i, row in df.iterrows():\n        sent = row[\"text\"]\n        sent_nopuncts = sent.translate(translator)\n        words_list = sent_nopuncts.strip().split()\n        filtered_words = [word for word in words_list if word not in stop_words and len(word) != 1] # also skip space from above translation\n        preprocessed_sentences.append(\" \".join(filtered_words))\n    df[\"text\"] = preprocessed_sentences\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-26T07:28:28.841892Z","iopub.execute_input":"2024-05-26T07:28:28.842199Z","iopub.status.idle":"2024-05-26T07:28:28.853068Z","shell.execute_reply.started":"2024-05-26T07:28:28.842161Z","shell.execute_reply":"2024-05-26T07:28:28.852242Z"},"trusted":true},"outputs":[],"execution_count":61},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/ucsd-dsc-258r-nlp/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/ucsd-dsc-258r-nlp/test.csv\")\n\ndf_train[\"text\"] = df_train[\"review\"]\ndf_test[\"text\"] = df_test[\"review\"]\n\ndf_train = preprocess_df(df_train)\ndf_test = preprocess_df(df_test)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T07:28:28.855700Z","iopub.execute_input":"2024-05-26T07:28:28.856149Z","iopub.status.idle":"2024-05-26T07:28:35.605606Z","shell.execute_reply.started":"2024-05-26T07:28:28.856123Z","shell.execute_reply":"2024-05-26T07:28:35.604452Z"},"trusted":true},"outputs":[],"execution_count":62},{"cell_type":"code","source":"#tokenize\ndf_train['tokens'] = df_train['text'].apply(word_tokenize)\ndf_test['tokens'] = df_test['text'].apply(word_tokenize)\n\n#build vocabulary\nsentences_train = df_train['tokens'].tolist()\nsentences_test = df_test['tokens'].tolist()\nword_counts, vocabulary, vocabulary_inv = build_vocab(sentences_train + sentences_test)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T07:28:35.607568Z","iopub.execute_input":"2024-05-26T07:28:35.607890Z","iopub.status.idle":"2024-05-26T07:29:42.767063Z","shell.execute_reply.started":"2024-05-26T07:28:35.607863Z","shell.execute_reply":"2024-05-26T07:29:42.765549Z"},"trusted":true},"outputs":[],"execution_count":63},{"cell_type":"code","source":"# prepare data\ntrain_inp_data = [[vocabulary.get(word, -1) for word in tokens] for tokens in sentences_train]\ntest_inp_data = [[vocabulary.get(word, -1) for word in tokens] for tokens in sentences_test]\n\n#embeddings\nembedding_model = get_embeddings(train_inp_data + test_inp_data, vocabulary_inv)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T07:29:42.768968Z","iopub.execute_input":"2024-05-26T07:29:42.769770Z","iopub.status.idle":"2024-05-26T07:31:34.211888Z","shell.execute_reply.started":"2024-05-26T07:29:42.769724Z","shell.execute_reply":"2024-05-26T07:31:34.210650Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Training Word2Vec model...\nSaving Word2Vec model embedding\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"def average_embeddings(embedding_model, inp_data, vocabulary_inv):\n    avg_embeddings = []\n    for tokens in inp_data:\n        vectors = [embedding_model.wv[vocabulary_inv[token]] for token in tokens if vocabulary_inv[token] in embedding_model.wv]\n        if vectors:\n            avg_embedding = np.mean(vectors, axis=0)\n        else:\n            avg_embedding = np.zeros(embedding_model.vector_size)\n        avg_embeddings.append(avg_embedding)\n    return np.array(avg_embeddings)\n\n\n# Average embeddings\ntrain_embeddings_avg = average_embeddings(embedding_model, train_inp_data, vocabulary_inv)\ntest_embeddings_avg = average_embeddings(embedding_model, test_inp_data, vocabulary_inv)\n\n#convert to sparse matrix\nembedding_train_dense = train_embeddings_avg\nembedding_test_dense = test_embeddings_avg","metadata":{"execution":{"iopub.status.busy":"2024-05-26T07:31:34.213652Z","iopub.execute_input":"2024-05-26T07:31:34.214096Z","iopub.status.idle":"2024-05-26T07:32:02.104590Z","shell.execute_reply.started":"2024-05-26T07:31:34.214055Z","shell.execute_reply":"2024-05-26T07:32:02.103427Z"},"trusted":true},"outputs":[],"execution_count":65},{"cell_type":"code","source":"#Sentiment Analysis\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom scipy.sparse import hstack, csr_matrix\n\nsid = SentimentIntensityAnalyzer()\ndef get_sentiment(text):\n    return sid.polarity_scores(text)['compound']\n\n# Get polarity of text\ndf_train['sentiment'] = df_train['text'].apply(get_sentiment)\ndf_test['sentiment'] = df_test['text'].apply(get_sentiment)\n\n#convert to numpy array and reshape\ntrain_sentiment = np.array(df_train['sentiment'], dtype=np.float64).reshape(-1, 1)\ntest_sentiment = np.array(df_test['sentiment'], dtype=np.float64).reshape(-1, 1)\n\ntrain_sentiment_dense = train_sentiment.reshape(-1, 1)\ntest_sentiment_dense = test_sentiment.reshape(-1, 1)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T07:32:02.106267Z","iopub.execute_input":"2024-05-26T07:32:02.106909Z","iopub.status.idle":"2024-05-26T07:34:12.285726Z","shell.execute_reply.started":"2024-05-26T07:32:02.106869Z","shell.execute_reply":"2024-05-26T07:34:12.284454Z"},"trusted":true},"outputs":[],"execution_count":66},{"cell_type":"code","source":"#TF-IDF Vectorization\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\ntfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\ntfidf_train = tfidf_vectorizer.fit_transform(df_train['text'])\ntfidf_test = tfidf_vectorizer.transform(df_test['text'])\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T07:34:12.287022Z","iopub.execute_input":"2024-05-26T07:34:12.287358Z","iopub.status.idle":"2024-05-26T07:35:04.477589Z","shell.execute_reply.started":"2024-05-26T07:34:12.287331Z","shell.execute_reply":"2024-05-26T07:35:04.476499Z"},"trusted":true},"outputs":[],"execution_count":67},{"cell_type":"code","source":"#combine\nfeatures_train = np.hstack([tfidf_train.toarray(), train_sentiment_dense, embedding_train_dense])\nfeatures_test = np.hstack([tfidf_test.toarray(), test_sentiment_dense, embedding_test_dense])\n\nprint(\"Shape of combined features_train:\", features_train.shape)\nprint(\"Shape of combined features_test:\", features_test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T07:35:04.479000Z","iopub.execute_input":"2024-05-26T07:35:04.479338Z","iopub.status.idle":"2024-05-26T07:35:06.168147Z","shell.execute_reply.started":"2024-05-26T07:35:04.479295Z","shell.execute_reply":"2024-05-26T07:35:06.167055Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Shape of combined features_train: (13144, 5101)\nShape of combined features_test: (10000, 5101)\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"#test train val split\nX_train, X_val, y_train, y_val = train_test_split(features_train, df_train['label'], test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T07:35:06.169349Z","iopub.execute_input":"2024-05-26T07:35:06.169711Z","iopub.status.idle":"2024-05-26T07:35:06.480665Z","shell.execute_reply.started":"2024-05-26T07:35:06.169684Z","shell.execute_reply":"2024-05-26T07:35:06.479717Z"},"trusted":true},"outputs":[],"execution_count":69},{"cell_type":"code","source":"from scipy.sparse import hstack\n\n# Train model\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\n# Predictions\nval_predictions = model.predict(X_val)\n\n# Evaluate the model\naccuracy = accuracy_score(y_val, val_predictions)\nf1 = f1_score(y_val, val_predictions, average='weighted')\n\nprint(\"Validation Accuracy:\", accuracy)\nprint(\"Validation F1 Score:\", f1)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T07:35:06.481953Z","iopub.execute_input":"2024-05-26T07:35:06.482323Z","iopub.status.idle":"2024-05-26T07:36:26.421470Z","shell.execute_reply.started":"2024-05-26T07:35:06.482293Z","shell.execute_reply":"2024-05-26T07:36:26.417367Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Validation Accuracy: 0.798022061620388\nValidation F1 Score: 0.7833301913121313\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"# Final predictions\ntest_predictions = model.predict(features_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T07:36:26.427624Z","iopub.execute_input":"2024-05-26T07:36:26.428180Z","iopub.status.idle":"2024-05-26T07:36:26.532398Z","shell.execute_reply.started":"2024-05-26T07:36:26.428130Z","shell.execute_reply":"2024-05-26T07:36:26.530854Z"},"trusted":true},"outputs":[],"execution_count":71},{"cell_type":"code","source":"#submission file\nsubmission_df = pd.DataFrame({\n    'Id': df_test['id'],  # Assuming 'id' is the column name for IDs in the test dataset\n    'Predicted': test_predictions\n})\nsubmission_df.to_csv(\"predicted.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T07:36:26.534334Z","iopub.execute_input":"2024-05-26T07:36:26.535596Z","iopub.status.idle":"2024-05-26T07:36:26.585917Z","shell.execute_reply.started":"2024-05-26T07:36:26.535543Z","shell.execute_reply":"2024-05-26T07:36:26.584143Z"},"trusted":true},"outputs":[],"execution_count":72},{"cell_type":"code","source":"#See predictions\nprint(\"Accuracy on Validation Set:\", accuracy_score(y_val, val_predictions))\nprint(\"Classification Report:\\n\", classification_report(y_val, val_predictions))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_val, val_predictions))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T07:36:26.588318Z","iopub.execute_input":"2024-05-26T07:36:26.589253Z","iopub.status.idle":"2024-05-26T07:36:26.776867Z","shell.execute_reply.started":"2024-05-26T07:36:26.589198Z","shell.execute_reply":"2024-05-26T07:36:26.775852Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Accuracy on Validation Set: 0.798022061620388\nClassification Report:\n                         precision    recall  f1-score   support\n\n        american (new)       0.58      0.40      0.47       281\namerican (traditional)       0.63      0.85      0.72       507\n          asian fusion       0.70      0.17      0.28        81\n        canadian (new)       0.43      0.20      0.27        99\n               chinese       0.88      0.94      0.91       327\n               italian       0.87      0.91      0.89       456\n              japanese       0.86      0.90      0.88       197\n         mediterranean       0.94      0.81      0.87       144\n               mexican       0.97      0.95      0.96       437\n                  thai       0.91      0.91      0.91       100\n\n              accuracy                           0.80      2629\n             macro avg       0.78      0.70      0.72      2629\n          weighted avg       0.79      0.80      0.78      2629\n\nConfusion Matrix:\n [[111 139   0   3   2  18   2   1   5   0]\n [ 43 430   0  10   4  15   0   1   3   1]\n [  2  10  14   5  22   4  17   2   0   5]\n [ 10  44   2  20   4  13   1   2   2   1]\n [  2   6   0   1 309   0   6   0   1   2]\n [ 14  21   0   3   2 415   0   1   0   0]\n [  4   8   1   0   6   0 177   0   1   0]\n [  4  10   0   2   0  12   0 116   0   0]\n [  3  15   0   3   0   0   1   0 415   0]\n [  0   0   3   0   3   1   1   1   0  91]]\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}